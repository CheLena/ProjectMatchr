{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "#Select stopwords corpora, punkt model, porter_stem model, averaged perceptron tagger\n",
    "import nltk\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import pickle\n",
    "import gensim\n",
    "import difflib\n",
    "from difflib import SequenceMatcher as sm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data and check metadata\n",
    "data = pd.read_csv(\"C:\\\\Users\\sugac_000\\Desktop\\Insight Data Science\\Dev-setups\\Project_Title_Data.csv\",encoding = 'ISO-8859-1')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View first 20 project titles\n",
    "data['project'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ensure project titles are strings that can be tokenized\n",
    "projects = str(data['project'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put lists of strings together into a single list\n",
    "project_titles = []\n",
    "for i in range (data.shape[0]):\n",
    "    project_titles.append(data.project[i])\n",
    "project_titles[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One way to tokenize; into a list of lists\n",
    "tokens_1 = [nltk.word_tokenize(str(item)) for item in project_titles]\n",
    "#flatten list of lists for frequency distribution\n",
    "tokens = [item for sublist in tokens_1 for item in sublist]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each row is a comma-separated list of the individual words of a single project. May still need to combine into a single list of all projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the most common 'words'?\n",
    "nltk.FreqDist(tokens).plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to get rid of non alphabetic characters (e.g., :, !, ., '), as well as stopwords (e.g., a, the, and, on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another, better way, to tokenize\n",
    "def text_preprocess(title): # perform tokenization, select noun, Lemmatization etc on a line text\n",
    "\n",
    "    rtext=[] # to collect all tokens   \n",
    "    for w, tag in nltk.pos_tag(nltk.word_tokenize(title.lower())):  # Tokenization of lowercased words\n",
    "        if tag in ['NN','NNP','NNS','VB','VBD','VBG','VBN','VBP','VBZ']:  # Keep only Nouns(project topics) and Verbs(project purpose)\n",
    "            rtext.append(w)\n",
    "     \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()  # Lemmatize\n",
    "    rtext = [wordnet_lemmatizer.lemmatize(w) for w in rtext]\n",
    "    \n",
    "    stemmer=PorterStemmer() # Stem\n",
    "    rtext = [stemmer.stem(w) for w in rtext]\n",
    "            \n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    rtext = [w for w in rtext if not w in stop_words]\n",
    "    \n",
    "    return rtext\n",
    "\n",
    "texts = []\n",
    "for i in range (0,len(tokens)-1):\n",
    "    texts.append(text_preprocess(tokens[i]))\n",
    "       \n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [item for sublist in texts for item in sublist]\n",
    "nltk.FreqDist(tokens).plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top term frequencies\n",
    "pd.value_counts(tokens)[:50]\n",
    "#1704 total terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDFModels(corpus_list):\n",
    "  tfidf = models.TfidfModel(corpus_list)\n",
    "  tfidf_corpus = tfidf[corpus_list]\n",
    "  return tfidf,tfidf_corpus\n",
    "\n",
    "#Create a bag of words from a list of text \n",
    "def GetVectors(evt_list,max_features=500):\n",
    "  vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = max_features) \n",
    "  in_features = vectorizer.fit_transform(evt_list)\n",
    "  in_features = in_features.toarray()\n",
    "  vocab = vectorizer.get_feature_names()\n",
    "  vocab = numpy.array(vocab)\n",
    "  return in_features,vocab\n",
    "\n",
    "TFIDFModels(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('clean_proj.txt','wb') as file1:\n",
    "    pickle.dump(texts,file1,protocol=2)\n",
    "\n",
    "cleaneddoc = pd.read_pickle('clean_proj.txt')\n",
    "\n",
    "model = gensim.models.Word2Vec(cleaneddoc, size=100, window=6, min_count=3, workers=1)\n",
    "model.save('W2Vmodel')\n",
    "\n",
    "model = gensim.models.Word2Vec.load('W2Vmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sorted vocabulary\n",
    "sorted(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity within titles\n",
    "model.wv.similarity('sentiment','analysi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity using numpy yields same result\n",
    "numpy.dot(model.wv['sentiment'], model.wv['analysi'])/(numpy.linalg.norm(model.wv['sentiment'])* numpy.linalg.norm(model.wv['analysi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('get','help')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('find')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('beer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similar_by_word('nyc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bokeh\n",
    "from sklearn.manifold import TSNE\n",
    "vocabulary = sorted(model.wv.vocab)\n",
    "emb_tuple = tuple([model[v] for v in vocabulary])\n",
    "X = np.vstack(emb_tuple)\n",
    "\n",
    "X_embedded = TSNE(n_components=2, init='pca', random_state=0).fit_transform(X)\n",
    "\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.models import ColumnDataSource, Range1d, LabelSet, Label\n",
    "\n",
    "source = ColumnDataSource(data=dict(x=list(X_embedded[:, 0]),\n",
    "                                    y=list(X_embedded[:, 1]),\n",
    "                                    words= vocabulary))\n",
    "\n",
    "p = figure(title='Word2Vec tSNE')\n",
    "p.scatter(x='x', y='y', size=2, source=source)\n",
    "labels = LabelSet(x='x', y='y', text='words', level='glyph',x_offset=5, y_offset=5, source=source, render_mode='canvas')\n",
    "p.add_layout(labels)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_embedded[:, 0], X_embedded[:, 1])\n",
    "for i, txt in enumerate(vocabulary):\n",
    "    ax.annotate(txt, (X_embedded[i, 0],X_embedded[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(1-model.wv.similarity('machin','learn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(1-model.wv.similarity('yelp','review'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(1-abs(model.wv.similarity('review','yelp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(1-abs(model.wv.similarity('travel','delay')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difflib.get_close_matches(project_titles[3], project_titles[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm(project_titles[63],project_titles[164]).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in tokens_1[63]:\n",
    "    print (\"score for: \" + \"beer\" + \" vs. \" + word + \" = \" + str(sm(None, \"beer\", word).ratio()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in tokens_1[63]:\n",
    "    if sm(None,\"recommend\",word).ratio() > 0.25:\n",
    "        print (\"score for: \" + \"recommend\" + \" vs. \" + word + \" = \" + str(sm(None, \"recommend\", word).ratio()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_match = [[print(\"score for: \" + \"find\" + \" vs. \" + word + \" = \" + str(sm(None, \"find\", word).ratio())) for word in tokens_1[i]  if sm(None,\"find\", word).ratio() > 0.70] for i in range (0,len(tokens_1)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_match = [[print(\"score for: \" + \"recommend\" + \" vs. \" + word + \" = \" + str(sm(None, \"recommend\", word).ratio())) for word in tokens_1[i]  if sm(None,\"recommend\", word).ratio() > 0.66] for i in range (0,len(tokens_1)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_match = [[print(\"score for: \" + \"twitter\" + \" vs. \" + word + \" = \" + str(sm(None, \"twitter\", word).ratio())) for word in tokens_1[i]  if sm(None,\"twitter\", word).ratio() > 0.66] for i in range (0,len(tokens_1)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.cluster import KMeans\n",
    "def clusterTitlesKmeans(titles):\n",
    "    taggeddocs   = []\n",
    "    tag2titlemap = {}\n",
    "    for index, i in enumerate(titles):\n",
    "        if len(i) > 2:  # Non empty titles\n",
    "            tag = u'SENT_{:d}'.format(index)\n",
    "            sentence = TaggedDocument(\n",
    "                words=gensim.utils.to_unicode(i).split(), tags=[tag])\n",
    "            tag2titlemap[tag] = i\n",
    "            taggeddocs.append(sentence)\n",
    "\n",
    "    model = Doc2Vec(\n",
    "        taggeddocs, dm=0, alpha=0.025, size=20, min_alpha=0.025, min_count=0)\n",
    "    print (\" \")\n",
    "    for epoch in range(60):\n",
    "        model.train(\n",
    "            taggeddocs, total_examples=model.corpus_count, epochs=model.iter)\n",
    "        model.alpha -= 0.002  # decrease the learning rate\n",
    "        model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "\n",
    "    dataSet = model.docvecs.doctag_syn0  \n",
    "    kmeansClustering = KMeans(n_clusters=10)\n",
    "    centroidIndx = kmeansClustering.fit_predict(dataSet)\n",
    "    def remove_non_ascii(text):\n",
    "        return ''.join(i for i in text if ord(i) < 128)\n",
    "    topic2wordsmap = {}\n",
    "    for i, val in enumerate(dataSet):\n",
    "        tag = model.docvecs.index_to_doctag(i)\n",
    "        topic = centroidIndx[i]\n",
    "        if topic in topic2wordsmap.keys():\n",
    "            for w in (tag2titlemap[tag].split()):\n",
    "                topic2wordsmap[topic].append(w)\n",
    "        else:\n",
    "            topic2wordsmap[topic] = []\n",
    "    for i in topic2wordsmap:\n",
    "        print(\"Topic {} has words: {}\".format(i + 1, ' '.join(remove_non_ascii(word) for word in topic2wordsmap[i][:20])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [[i.lower() for i in w] for w in tokens_1 if w not in stopwords.words('english') and len(w) > 2 ]\n",
    "titles_1 = [[bytes(str(i),'utf-8') for i in w] for w in titles]\n",
    "clusterTitlesKmeans(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_preprocess(title): # perform tokenization, select noun, Lemmatization etc on a line text\n",
    "\n",
    "    rtext=[] # to collect all tokens   \n",
    "    for w, tag in nltk.pos_tag(nltk.word_tokenize(title.lower())):  # Tokenization of lowercased words\n",
    "        if tag in ['NN','NNS']:  # Keep only Nouns(project topics) \n",
    "            rtext.append(w)\n",
    "     \n",
    "    #wordnet_lemmatizer = WordNetLemmatizer()  # Lemmatize\n",
    "    #rtext = [wordnet_lemmatizer.lemmatize(w) for w in rtext]  \n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    rtext = [w for w in rtext if not w in stop_words]\n",
    "    \n",
    "    stemmer=PorterStemmer() # Stem\n",
    "    rtext = [stemmer.stem(w) for w in rtext]\n",
    "    \n",
    "    return rtext\n",
    "\n",
    "nouns_only = []\n",
    "for i in range (0,len(tokens)-1):\n",
    "    nouns_only.append(noun_preprocess(tokens[i]))\n",
    "       \n",
    "noun_tokens = [item for sublist in nouns_only for item in sublist]\n",
    "nltk.FreqDist(noun_tokens).plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterTitlesKmeans(noun_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_preprocess(title): # perform tokenization, select noun, Lemmatization etc on a line text\n",
    "\n",
    "    rtext=[] # to collect all tokens   \n",
    "    for w, tag in nltk.pos_tag(nltk.word_tokenize(title.lower())):  # Tokenization of lowercased words\n",
    "        if tag in ['VB','VBD','VBG','VBN','VBP','VBZ']:  # Keep only Verbs(project actions) \n",
    "            rtext.append(w)\n",
    "     \n",
    "    #wordnet_lemmatizer = WordNetLemmatizer()  # Lemmatize\n",
    "    #rtext = [wordnet_lemmatizer.lemmatize(w) for w in rtext]  \n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    rtext = [w for w in rtext if not w in stop_words]\n",
    "    \n",
    "    stemmer=PorterStemmer() # Stem\n",
    "    rtext = [stemmer.stem(w) for w in rtext]\n",
    "    \n",
    "    return rtext\n",
    "\n",
    "verbs_only = []\n",
    "for i in range (0,len(tokens)-1):\n",
    "    verbs_only.append(verb_preprocess(tokens[i]))\n",
    "       \n",
    "verb_tokens = [item for sublist in verbs_only for item in sublist]\n",
    "nltk.FreqDist(verb_tokens).plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterTitlesKmeans(verb_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_docs(titles):\n",
    "    taggeddocs   = []\n",
    "    tag2titlemap = {}\n",
    "    for index, i in enumerate(titles):\n",
    "        if len(i) > 2:  # Non empty titles\n",
    "            tag = u'SENT_{:d}'.format(index)\n",
    "            sentence = TaggedDocument(words=gensim.utils.to_unicode(i).split(), tags=[tag])\n",
    "            tag2titlemap[tag] = i\n",
    "            taggeddocs.append(sentence)\n",
    "    return taggeddocs\n",
    "            \n",
    "tagged_docs(project_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top bi-grams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokens, 6)\n",
    "finder.apply_freq_filter(3)\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "texts = [text for text in project_titles if len(text) > 2]\n",
    "doc_clean = [clean(doc).split() for doc in texts]\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "ldamodel = models.ldamodel.LdaModel(doc_term_matrix, num_topics=8, id2word = \n",
    "dictionary, passes=3)\n",
    "for topic in ldamodel.show_topics(num_topics=8, formatted=False, num_words=4):\n",
    "    print(\"Topic {}: Words: \".format(topic[0]))\n",
    "    topicwords = [w for (w, val) in topic[1]]\n",
    "    print(topicwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "     return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray()\n",
    "cos_similarity(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity(tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(cos_similarity(tokens), cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity(tokens)[:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"machin\",\"learn\")*model.wv.similarity(\"recommend\",\"engin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
